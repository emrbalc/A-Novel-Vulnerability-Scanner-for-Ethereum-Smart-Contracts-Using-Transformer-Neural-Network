!pip install tokenizers

# CELL 1: Import necessary libraries
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# CELL 2: CONFIG
# CELL 1: Define the configuration for the model

config = {
    "num_words": 150,  # Number of distinct input tokens
    "max_length": 1024,  # Max length of string sequences
    "embedding_dim": 64,  # Dimension of the dense embedding
    "batch_size": 128,  # Number of samples per gradient update
    "epochs": 100,  # Number of times to iterate over the training data arrays
    "learning_rate": 0.001,  # Learning rate for the optimizer
    "head_size": 128,  # Dimensionality of the output space of the key, query and value inputs
    "num_heads": 4,  # Number of attention heads
    "ff_dim": 4*128,  # Number of units in the hidden layers of the feed forward network
    "dropout": 0.1,  # Fraction of the input units to drop
}

# CELL 3: Read data

data = pd.read_csv('c_new_contracts_updated.csv', usecols=['ADDRESS', 'OPCODE', 'CATEGORY'], sep=';')
# CELL 4: Create the Tokenizer and fit on texts
tokenizer = Tokenizer(num_words=config["num_words"], lower=False)
tokenizer.fit_on_texts(data["OPCODE"].values)
# CELL 5: Convert text to sequences
sequences = tokenizer.texts_to_sequences(data["OPCODE"].values)
# CELL 6: Pad sequences
X = pad_sequences(sequences, maxlen=config["max_length"])
# CELL 7: Prepare labels
labels = data['CATEGORY'].apply(lambda x: 0 if x == '1 0 0 0' else 1)
y = to_categorical(labels)

# CELL 8: Split data into train, validation, and test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)

# CELL 9: Define the Transformer model
from tensorflow.keras import layers
import tensorflow as tf

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res

inputs = layers.Input(shape=(config["max_length"],))
x = layers.Embedding(input_dim=config["num_words"], output_dim=config["embedding_dim"])(inputs)
x = transformer_encoder(x, head_size=config["head_size"], num_heads=config["num_heads"], ff_dim=config["ff_dim"])
x = layers.GlobalAveragePooling1D()(x)
outputs = layers.Dense(2, activation="softmax")(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

#optimizer = Adam(learning_rate=config["learning_rate"])
#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])


model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['acc'])

# CELL 10: Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val))