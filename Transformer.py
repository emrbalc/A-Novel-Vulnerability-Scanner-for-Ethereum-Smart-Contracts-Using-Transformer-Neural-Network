!pip install tokenizers

# CELL 1: Import necessary libraries
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# CELL 2: CONFIG
# CELL 1: Define the configuration for the model

config = {
    "num_words": 150,  # Number of distinct input tokens
    "max_length": 1024,  # Max length of string sequences
    "embedding_dim": 64,  # Dimension of the dense embedding
    "batch_size": 128,  # Number of samples per gradient update
    "epochs": 100,  # Number of times to iterate over the training data arrays
    "learning_rate": 0.001,  # Learning rate for the optimizer
    "head_size": 128,  # Dimensionality of the output space of the key, query and value inputs
    "num_heads": 4,  # Number of attention heads
    "ff_dim": 4*128,  # Number of units in the hidden layers of the feed forward network
    "dropout": 0.1,  # Fraction of the input units to drop
}

# CELL 3: Read data

data = pd.read_csv('c_new_contracts_updated.csv', usecols=['ADDRESS', 'OPCODE', 'CATEGORY'], sep=';')
# CELL 4: Create the Tokenizer and fit on texts
tokenizer = Tokenizer(num_words=config["num_words"], lower=False)
tokenizer.fit_on_texts(data["OPCODE"].values)
# CELL 5: Convert text to sequences
sequences = tokenizer.texts_to_sequences(data["OPCODE"].values)
# CELL 6: Pad sequences
X = pad_sequences(sequences, maxlen=config["max_length"])
# CELL 7: Prepare labels
labels = data['CATEGORY'].apply(lambda x: 0 if x == '1 0 0 0' else 1)
y = to_categorical(labels)

# CELL 8: Split data into train, validation, and test
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)

# CELL 9: Define the Transformer model
from tensorflow.keras import layers
import tensorflow as tf

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res

inputs = layers.Input(shape=(config["max_length"],))
x = layers.Embedding(input_dim=config["num_words"], output_dim=config["embedding_dim"])(inputs)
x = transformer_encoder(x, head_size=config["head_size"], num_heads=config["num_heads"], ff_dim=config["ff_dim"])
x = layers.GlobalAveragePooling1D()(x)
outputs = layers.Dense(2, activation="softmax")(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

#optimizer = Adam(learning_rate=config["learning_rate"])
#model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])


model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['acc'])

# CELL 10: Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val))

# CELL 12: Plotting

import matplotlib.pyplot as plt

# Save the model history in a file for later use
with open('c_train_c_test_history.txt', 'w') as file:
     file.write(str(history.history))

# Plot training & validation accuracy values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper left')

plt.tight_layout()
plt.savefig('c_train_c_test_plot.png')  # saves the plot to a file
plt.show()

model.save_weights('c_train_c_test_weights.h5')

# Assuming that test_data has already been loaded
test_data = pd.read_csv('c_new_contracts_updated.csv', usecols=['ADDRESS', 'OPCODE', 'CATEGORY'], sep=';')
test_data = test_data.sample(frac=0.2, random_state=42)

# Tokenize the OPCODE
sequences = tokenizer.texts_to_sequences(test_data["OPCODE"].values)
X_test = pad_sequences(sequences, maxlen=config["max_length"])

# Convert the categories to binary labels
y_test = test_data['CATEGORY'].apply(lambda x: 0 if x == '1 0 0 0' else 1)

# Convert to categorical
y_test = to_categorical(y_test)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)

print('Test Loss: {}'.format(test_loss))
print('Test Accuracy: {}'.format(test_acc))

# Get predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Convert y_test back from one-hot encoding
y_test_classes = np.argmax(y_test, axis=1)

# Generate confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test_classes, y_pred_classes)

print("Confusion Matrix: ")
print(cm)

print("Classification Report: ")
print(classification_report(y_test_classes, y_pred_classes, target_names=['non vulnerable', 'vulnerable']))

from sklearn.metrics import accuracy_score, log_loss
import numpy as np
test_data = pd.read_csv('c_new_contracts_updated.csv', usecols=['ADDRESS', 'OPCODE', 'CATEGORY'], sep=';')

test_data = test_data.sample(frac=0.2, random_state=42)

n = test_data[test_data.CATEGORY == '1 0 0 0']
s = test_data[test_data.CATEGORY == '0 1 0 0']
p = test_data[test_data.CATEGORY == '0 0 1 0']
g = test_data[test_data.CATEGORY == '0 0 0 1']

catarr = [n, s, p, g]
cat_names = ['non vulnerable', 'suicidial', 'prodigal', 'greedy']

for i, test_cat in enumerate(catarr):
  try:
    print("\nTesting for:", cat_names[i])

    # Process OPCODE data
    tokenizer = Tokenizer(num_words=config["num_words"], lower=False)
    tokenizer.fit_on_texts(test_cat["OPCODE"].values)
    sequences = tokenizer.texts_to_sequences(test_cat["OPCODE"].values)
    x_test_class = np.array(pad_sequences(sequences, maxlen=config["max_length"]))

    # For non vulnerable class, labels are all zeros. For other classes, labels are all ones.
    y_test_class = np.zeros(len(test_cat)) if i == 0 else np.ones(len(test_cat))

    # Evaluate the model
    y_pred = model.predict(x_test_class)
    y_pred_class = np.round(y_pred[:, 1]).astype(int)

    # Generate confusion matrix
    cm = confusion_matrix(y_test_class, y_pred_class)

    print(f"Test Accuracy: {accuracy_score(y_test_class, y_pred_class)}")
    print("Confusion Matrix: ")
    print(cm)
    print("Classification Report: ")
    print(classification_report(y_test_class, y_pred_class))

  except Exception as e:
    print(f"An error occurred while processing the {cat_names[i]} class. Error: {str(e)}")

